{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "KxBuxJ6RvwSe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funcionalidade das bibliotecas que foram utilizadas:\n",
        "\n",
        "- Numpy: Facilita algumas operações e manipulações matematicas, e permite a captura da forma vetorial das imagens.\n",
        "- os: Fornece algumas funções de manipulação do proprio sistema operacional que foram uteis para testes feitos antes da integração com a base de dados.\n",
        "- cv2: Permite o redimensionamento das imagens.\n",
        "- sklearn: Biblioteca de aprendizagem de maquina da qual foi utilizada as funções que implementaram o perceptron\n",
        "- TensorFlow\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Considerando uma implementação de teste sem integração com a base de dados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "É pego a lista de diretórios do caminho atual, e então atribuido a uma lista. Esses diretórios são as categorias de animais que serão avaliadas, no caso, gatos e cachorros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "UA42YrR5v8QZ",
        "outputId": "3106339f-19b1-45e9-bd29-46f9bb39e06a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cats', 'dogs']"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Animals_types = os.listdir('Animals//')\n",
        "Animals_types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Então, para cada diretório que foi descoberto é feita uma varredura, e atribuida cada imagem que está no formato jpg para a lista de imagens, e a sua categoria correspondente para a lista de labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CAT1.jpg', 'CAT2.jpg']\n",
            "['DOG 1.jpg', 'DOG 2.jpg']\n"
          ]
        }
      ],
      "source": [
        "path = 'Animals//'\n",
        "\n",
        "sizeExpected = 60\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in Animals_types:\n",
        "    data_path = path + str(i)\n",
        "    filenames = [i for i in os.listdir(data_path) if i.endswith('.jpg')]\n",
        "    print(filenames)\n",
        "    for f in filenames:\n",
        "        img = cv2.imread(data_path + '/' + f)\n",
        "        img = cv2.resize(img,(sizeExpected, sizeExpected))\n",
        "        images.append(img)\n",
        "        labels.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com isso, pode-se converter as imagens em questão para sua forma vetorial, garantir que estejam no formato correto (Float32) e sobre a escala de cores RGB (255)\n",
        "\n",
        "Com a função shape podemos ver que o primeiro elemento da 4-tupla é a quantidade de imagens que foram pegas, os proximos dois são a dimensão que foi escolhida para análise, e o ultimo elemento indica que se trata de uma imagem colorida (RGB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 60, 60, 3)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images = np.array(images)\n",
        "images = images.astype('float32')/255\n",
        "images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com as funções fornecidas pela biblioteca sklearn podemos começar a criação e atribuição de labels para o treinamento. Dessa forma, a variavel x recebe a lista de imagens pegas, e a y recebe a lista binária de labels (o que permite que as labels sejam \"entendidas\" pelo computador), onde no nosso caso, seria:\n",
        "- 0 para Gatos\n",
        "- 1 para Cachorros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "x = images\n",
        "y = labels\n",
        "y_labelEncoder = LabelEncoder()\n",
        "y = y_labelEncoder.fit_transform(y)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesse momento é separado as labels em duas classes já que precisamos de duas colunas de classificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "y = y.reshape(-1,1)\n",
        "\n",
        "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "y = ct.fit_transform(y)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste momento é feita a separação entre treino e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "x,Y = shuffle(x,y, random_state=1)\n",
        "train_x, test_x, train_y, test_y = train_test_split(images, y, test_size=0.20, random_state=415)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 60, 60, 3)\n",
            "(3, 2)\n",
            "(1, 60, 60, 3)\n",
            "(1, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o objetivo é utilizar um perceptron para resolver o problema da classificação, precisamos transformar o formato da imagem para apenas uma dimensão. Dessa forma, temos que:\n",
        "\n",
        "Nova dimensão = (n, r * r * 3)\n",
        "\n",
        "Sendo o n o valor do primeiro elemento da 4-tupla da célula anterior (o numero de imagens do treino e do teste), e r o numéro que redimensionamos a imagem anteriormente.\n",
        "\n",
        "Logo:\n",
        "- Para o treino:    Nova dimensão = (3, 10800)\n",
        "- Para o teste:     Nova dimensão = (1, 10800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 10800)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 10800)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_x=np.reshape(train_x,(3,10800))\n",
        "print(train_x.shape)\n",
        "\n",
        "test_x=np.reshape(test_x,(1,10800))\n",
        "test_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "training_epochs = 100\n",
        "n_dim=60\n",
        "n_class=2\n",
        "\n",
        "\n",
        "Perceptron_x = tf.compat.v1.placeholder(tf.float32, [None, 10800])\n",
        "Perceptron_W = tf.Variable(tf.zeros([10800,n_class]))\n",
        "Perceptron_b = tf.Variable(tf.zeros([n_class]))\n",
        "Perceptron_y = tf.compat.v1.placeholder(tf.float32,[None,n_class]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "init = tf.compat.v1.global_variables_initializer()\n",
        "pred = tf.nn.softmax(tf.matmul(Perceptron_x, Perceptron_W)+ Perceptron_b) # perceptron\n",
        " \n",
        "error_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Perceptron_y)) \n",
        "training_step = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(error_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "\n",
        "cost_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch :  0  -  cost:  0.31327066\n",
            "epoch :  1  -  cost:  0.31327063\n",
            "epoch :  2  -  cost:  0.3132706\n",
            "epoch :  3  -  cost:  0.3132706\n",
            "epoch :  4  -  cost:  0.31327057\n",
            "epoch :  5  -  cost:  0.3132705\n",
            "epoch :  6  -  cost:  0.3132705\n",
            "epoch :  7  -  cost:  0.31327045\n",
            "epoch :  8  -  cost:  0.31327045\n",
            "epoch :  9  -  cost:  0.31327042\n",
            "epoch :  10  -  cost:  0.3132704\n",
            "epoch :  11  -  cost:  0.31327036\n",
            "epoch :  12  -  cost:  0.31327036\n",
            "epoch :  13  -  cost:  0.31327033\n",
            "epoch :  14  -  cost:  0.3132703\n",
            "epoch :  15  -  cost:  0.3132703\n",
            "epoch :  16  -  cost:  0.31327024\n",
            "epoch :  17  -  cost:  0.31327024\n",
            "epoch :  18  -  cost:  0.31327024\n",
            "epoch :  19  -  cost:  0.3132702\n",
            "epoch :  20  -  cost:  0.31327018\n",
            "epoch :  21  -  cost:  0.31327018\n",
            "epoch :  22  -  cost:  0.31327012\n",
            "epoch :  23  -  cost:  0.31327012\n",
            "epoch :  24  -  cost:  0.3132701\n",
            "epoch :  25  -  cost:  0.31327006\n",
            "epoch :  26  -  cost:  0.31327003\n",
            "epoch :  27  -  cost:  0.31327\n",
            "epoch :  28  -  cost:  0.31327\n",
            "epoch :  29  -  cost:  0.31326997\n",
            "epoch :  30  -  cost:  0.31326997\n",
            "epoch :  31  -  cost:  0.31326994\n",
            "epoch :  32  -  cost:  0.3132699\n",
            "epoch :  33  -  cost:  0.3132699\n",
            "epoch :  34  -  cost:  0.31326988\n",
            "epoch :  35  -  cost:  0.31326985\n",
            "epoch :  36  -  cost:  0.31326985\n",
            "epoch :  37  -  cost:  0.31326982\n",
            "epoch :  38  -  cost:  0.31326976\n",
            "epoch :  39  -  cost:  0.31326976\n",
            "epoch :  40  -  cost:  0.31326976\n",
            "epoch :  41  -  cost:  0.31326973\n",
            "epoch :  42  -  cost:  0.3132697\n",
            "epoch :  43  -  cost:  0.3132697\n",
            "epoch :  44  -  cost:  0.31326964\n",
            "epoch :  45  -  cost:  0.31326964\n",
            "epoch :  46  -  cost:  0.31326964\n",
            "epoch :  47  -  cost:  0.31326962\n",
            "epoch :  48  -  cost:  0.3132696\n",
            "epoch :  49  -  cost:  0.31326953\n",
            "epoch :  50  -  cost:  0.3132696\n",
            "epoch :  51  -  cost:  0.3132696\n",
            "epoch :  52  -  cost:  0.31326953\n",
            "epoch :  53  -  cost:  0.31326953\n",
            "epoch :  54  -  cost:  0.3132695\n",
            "epoch :  55  -  cost:  0.31326947\n",
            "epoch :  56  -  cost:  0.31326947\n",
            "epoch :  57  -  cost:  0.31326947\n",
            "epoch :  58  -  cost:  0.31326944\n",
            "epoch :  59  -  cost:  0.3132694\n",
            "epoch :  60  -  cost:  0.3132694\n",
            "epoch :  61  -  cost:  0.31326938\n",
            "epoch :  62  -  cost:  0.31326935\n",
            "epoch :  63  -  cost:  0.31326935\n",
            "epoch :  64  -  cost:  0.31326932\n",
            "epoch :  65  -  cost:  0.3132693\n",
            "epoch :  66  -  cost:  0.3132693\n",
            "epoch :  67  -  cost:  0.31326926\n",
            "epoch :  68  -  cost:  0.31326923\n",
            "epoch :  69  -  cost:  0.31326926\n",
            "epoch :  70  -  cost:  0.31326923\n",
            "epoch :  71  -  cost:  0.31326923\n",
            "epoch :  72  -  cost:  0.3132692\n",
            "epoch :  73  -  cost:  0.3132692\n",
            "epoch :  74  -  cost:  0.31326917\n",
            "epoch :  75  -  cost:  0.31326914\n",
            "epoch :  76  -  cost:  0.31326917\n",
            "epoch :  77  -  cost:  0.31326914\n",
            "epoch :  78  -  cost:  0.31326914\n",
            "epoch :  79  -  cost:  0.3132691\n",
            "epoch :  80  -  cost:  0.3132691\n",
            "epoch :  81  -  cost:  0.3132691\n",
            "epoch :  82  -  cost:  0.31326905\n",
            "epoch :  83  -  cost:  0.31326905\n",
            "epoch :  84  -  cost:  0.31326902\n",
            "epoch :  85  -  cost:  0.313269\n",
            "epoch :  86  -  cost:  0.313269\n",
            "epoch :  87  -  cost:  0.31326905\n",
            "epoch :  88  -  cost:  0.31326902\n",
            "epoch :  89  -  cost:  0.313269\n",
            "epoch :  90  -  cost:  0.313269\n",
            "epoch :  91  -  cost:  0.31326893\n",
            "epoch :  92  -  cost:  0.31326893\n",
            "epoch :  93  -  cost:  0.31326893\n",
            "epoch :  94  -  cost:  0.31326893\n",
            "epoch :  95  -  cost:  0.31326893\n",
            "epoch :  96  -  cost:  0.3132689\n",
            "epoch :  97  -  cost:  0.3132689\n",
            "epoch :  98  -  cost:  0.3132689\n",
            "epoch :  99  -  cost:  0.31326887\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(training_epochs):\n",
        "    sess.run(training_step,feed_dict={Perceptron_x:train_x,Perceptron_y:train_y})\n",
        "    cost = sess.run(error_function,feed_dict={Perceptron_x: train_x,Perceptron_y: train_y})\n",
        "    cost_history = np.append(cost_history,cost)\n",
        "    print('epoch : ', epoch,  ' - ', 'cost: ', cost)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image = load_img('algo2.jpg', target_size=(60,60))\n",
        "image = np.array(image)\n",
        "\n",
        "image.shape\n",
        "image = np.reshape(image,(1,10800))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1.]]\n",
            "Cachorro\n"
          ]
        }
      ],
      "source": [
        "result = sess.run(pred, feed_dict={Perceptron_x: image})\n",
        "print(result)\n",
        "\n",
        "if(result[0][0] > result[0][1]):\n",
        "    print(\"Gato\")\n",
        "else:\n",
        "    print(\"Cachorro\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Classical-Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
